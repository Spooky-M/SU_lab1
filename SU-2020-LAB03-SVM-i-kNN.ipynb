{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Sveučilište u Zagrebu  \n",
    "Fakultet elektrotehnike i računarstva  \n",
    "  \n",
    "## Strojno učenje 2020/2021  \n",
    "http://www.fer.unizg.hr/predmet/su"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "------------------------------\n",
    "\n",
    "### Laboratorijska vježba 3: Stroj potpornih vektora i algoritam k-najbližih susjeda\n",
    "\n",
    "*Verzija: 0.5  \n",
    "Zadnji put ažurirano: 30. studenog 2020.*\n",
    "\n",
    "(c) 2015-2020 Jan Šnajder, Domagoj Alagić  \n",
    "\n",
    "Rok za predaju: **7. prosinca 2020. u 06:00h**  \n",
    "\n",
    "------------------------------"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Upute\n",
    "\n",
    "Treća laboratorijska vježba sastoji se od sedam zadataka. U nastavku slijedite upute navedene u ćelijama s tekstom. Rješavanje vježbe svodi se na **dopunjavanje ove bilježnice**: umetanja ćelije ili više njih **ispod** teksta zadatka, pisanja odgovarajućeg kôda te evaluiranja ćelija. \n",
    "\n",
    "Osigurajte da u potpunosti **razumijete** kôd koji ste napisali. Kod predaje vježbe, morate biti u stanju na zahtjev asistenta (ili demonstratora) preinačiti i ponovno evaluirati Vaš kôd. Nadalje, morate razumjeti teorijske osnove onoga što radite, u okvirima onoga što smo obradili na predavanju. Ispod nekih zadataka možete naći i pitanja koja služe kao smjernice za bolje razumijevanje gradiva (**nemojte pisati** odgovore na pitanja u bilježnicu). Stoga se nemojte ograničiti samo na to da riješite zadatak, nego slobodno eksperimentirajte. To upravo i jest svrha ovih vježbi.\n",
    "\n",
    "Vježbe trebate raditi **samostalno** ili u **tandemu**. Možete se konzultirati s drugima o načelnom načinu rješavanja, ali u konačnici morate sami odraditi vježbu. U protivnome vježba nema smisla."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.datasets import make_classification\n",
    "%pylab inline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_2d_clf_problem(X, y, h=None):\n",
    "    '''\n",
    "    Plots a two-dimensional labeled dataset (X,y) and, if function h(x) is given, \n",
    "    the decision surfaces.\n",
    "    '''\n",
    "    assert X.shape[1] == 2, \"Dataset is not two-dimensional\"\n",
    "    if h!=None : \n",
    "        # Create a mesh to plot in\n",
    "        r = 0.03  # mesh resolution\n",
    "        x_min, x_max = X[:, 0].min() - 1, X[:, 0].max() + 1\n",
    "        y_min, y_max = X[:, 1].min() - 1, X[:, 1].max() + 1\n",
    "        xx, yy = np.meshgrid(np.arange(x_min, x_max, r),\n",
    "                             np.arange(y_min, y_max, r))\n",
    "        XX=np.c_[xx.ravel(), yy.ravel()]\n",
    "        try:\n",
    "            Z_test = h(XX)\n",
    "            if Z_test.shape == ():\n",
    "                # h returns a scalar when applied to a matrix; map explicitly\n",
    "                Z = np.array(list(map(h,XX)))\n",
    "            else :\n",
    "                Z = Z_test\n",
    "        except ValueError:\n",
    "            # can't apply to a matrix; map explicitly\n",
    "            Z = np.array(list(map(h,XX)))\n",
    "        # Put the result into a color plot\n",
    "        Z = Z.reshape(xx.shape)\n",
    "        plt.contourf(xx, yy, Z, cmap=plt.cm.Pastel1)\n",
    "\n",
    "    # Plot the dataset\n",
    "    plt.scatter(X[:,0],X[:,1], c=y, cmap=plt.cm.tab20b, marker='o', s=50);\n",
    "    \n",
    "def plot_2d_svc_problem(X, y, svc=None):\n",
    "    '''\n",
    "    Plots a two-dimensional labeled dataset (X,y) and, if SVC object is given, \n",
    "    the decision surfaces (with margin as well).\n",
    "    '''\n",
    "    assert X.shape[1] == 2, \"Dataset is not two-dimensional\"\n",
    "    if svc!=None : \n",
    "        # Create a mesh to plot in\n",
    "        r = 0.03  # mesh resolution\n",
    "        x_min, x_max = X[:, 0].min() - 1, X[:, 0].max() + 1\n",
    "        y_min, y_max = X[:, 1].min() - 1, X[:, 1].max() + 1\n",
    "        xx, yy = np.meshgrid(np.arange(x_min, x_max, r),\n",
    "                             np.arange(y_min, y_max, r))\n",
    "        XX=np.c_[xx.ravel(), yy.ravel()]\n",
    "        Z = np.array([svc_predict(svc, x) for x in XX])\n",
    "        # Put the result into a color plot\n",
    "        Z = Z.reshape(xx.shape)\n",
    "        plt.contourf(xx, yy, Z, cmap=plt.cm.Pastel1)\n",
    "\n",
    "    # Plot the dataset\n",
    "    plt.scatter(X[:,0],X[:,1], c=y, cmap=plt.cm.Paired, marker='o', s=50)\n",
    "\n",
    "def svc_predict(svc, x) : \n",
    "    h = svc.decision_function([x])\n",
    "    if np.isclose(h, 0, atol=0.03):\n",
    "        return 5\n",
    "    elif (h >= -1 and h < -0.03) or (h > 0.03 and h <= 1):\n",
    "        return 0.5\n",
    "    else: \n",
    "        return max(-1, min(1, h))\n",
    "    \n",
    "def plot_error_surface(err, c_range=(0,5), g_range=(0,5)):\n",
    "    c1, c2 = c_range[0], c_range[1]\n",
    "    g1, g2 = g_range[0], g_range[1]\n",
    "    plt.xticks(range(0,g2-g1+1,5),range(g1,g2+1,5)); plt.xlabel(\"gamma\")\n",
    "    plt.yticks(range(0,c2-c1+1,5),range(c1,c2+1,5)); plt.ylabel(\"C\")\n",
    "    p = plt.contour(err);\n",
    "    plt.imshow(1-err, interpolation='bilinear', origin='lower',cmap=plt.cm.gray)\n",
    "    plt.clabel(p, inline=1, fontsize=10)\n",
    "    \n",
    "def knn_eval(n_instances=100, n_features=2, n_classes=2, n_informative=2, \n",
    "             test_size=0.3, k_range=(1, 20), n_trials=40):\n",
    "    \n",
    "    train_errors = []\n",
    "    test_errors = []\n",
    "    ks = list(range(k_range[0], k_range[1] + 1))\n",
    "\n",
    "    for i in range(0, n_trials):\n",
    "        X, y = make_classification(n_instances, n_features, n_classes=n_classes, \n",
    "                                   n_informative=n_informative, n_redundant=0, n_clusters_per_class=1)\n",
    "        X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=test_size)\n",
    "        train = []\n",
    "        test = []\n",
    "        for k in ks:\n",
    "            knn = KNeighborsClassifier(n_neighbors=k)\n",
    "            knn.fit(X_train, y_train)\n",
    "            train.append(1 - knn.score(X_train, y_train))\n",
    "            test.append(1 - knn.score(X_test, y_test))\n",
    "        train_errors.append(train)\n",
    "        test_errors.append(test)\n",
    "        \n",
    "    train_errors = np.mean(np.array(train_errors), axis=0)\n",
    "    test_errors = np.mean(np.array(test_errors), axis=0)\n",
    "    best_k = ks[np.argmin(test_errors)]\n",
    "    \n",
    "    return ks, best_k, train_errors, test_errors"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1. Klasifikator stroja potpornih vektora (SVM)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### (a)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Upoznajte se s razredom [`svm.SVC`](http://scikit-learn.org/stable/modules/generated/sklearn.svm.SVC.html), koja ustvari implementira sučelje prema implementaciji [`libsvm`](http://www.csie.ntu.edu.tw/~cjlin/libsvm/). Primijenite model `SVC` s linearnom jezgrenom funkcijom (tj. bez preslikavanja primjera u prostor značajki) na skup podataka `seven` (dan niže) s $N=7$ primjera. Ispišite koeficijente $w_0$ i $\\mathbf{w}$. Ispišite dualne koeficijente i potporne vektore. Završno, koristeći funkciju `plot_2d_svc_problem` iscrtajte podatke, decizijsku granicu i marginu. Funkcija prima podatke, oznake i klasifikator (objekt klase `SVC`). "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.svm import SVC\n",
    "\n",
    "seven_X = np.array([[2,1], [2,3], [1,2], [3,2], [5,2], [5,4], [6,3]])\n",
    "seven_y = np.array([1, 1, 1, 1, -1, -1, -1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "svc = SVC(kernel=\"linear\").fit(seven_X, seven_y)\n",
    "\n",
    "w = svc.coef_[0]\n",
    "w0 = svc.intercept_\n",
    "dualni_koef = svc.dual_coef_[0]\n",
    "potporni_vektori = svc.support_vectors_\n",
    "\n",
    "norma = 2 * (1/norm(w))\n",
    "\n",
    "print(\"w0: {}\\nw: {}\\ndualni koeficijenti: {}\\npotporni vektori:\\n{}\\nširina margine: {}\\n\".format(w0, w, dualni_koef, potporni_vektori, norma))\n",
    "\n",
    "figure(num=None, figsize=(8, 6), dpi=80, facecolor='w', edgecolor='k')\n",
    "plot_2d_svc_problem(seven_X, seven_y, svc)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Q:** Koji primjeri su potporni vektori i zašto?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### (b)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Definirajte funkciju `hinge(model, x, y)` koja izračunava gubitak zglobnice modela SVM na primjeru `x`. Izračunajte gubitke modela naučenog na skupu `seven` za primjere $\\mathbf{x}^{(2)}=(3,2)$ i $\\mathbf{x}^{(1)}=(3.5,2)$ koji su označeni pozitivno ($y=1$) te za $\\mathbf{x}^{(3)}=(4,2)$ koji je označen negativno ($y=-1$). Također, izračunajte prosječni gubitak SVM-a na skupu `seven`. Uvjerite se da je rezultat identičan onome koji biste dobili primjenom ugrađene funkcije [`metrics.hinge_loss`](http://scikit-learn.org/stable/modules/generated/sklearn.metrics.hinge_loss.html)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import hinge_loss\n",
    "\n",
    "hinge = lambda model, x, y: max(0, 1 - y * model.decision_function(x))\n",
    "\n",
    "X = np.array(\n",
    "    [[3,2],\n",
    "    [3.5,2],\n",
    "    [4,2]])\n",
    "y = np.array([1,1,-1])\n",
    "\n",
    "for i, (xi, yi) in enumerate(zip(X, y)):\n",
    "    print(\"Iteracija {}:\\n\\tx={}\\n\\ty={}\\n\\tL={}\\n\".format(i+1, xi, yi, hinge(svc, xi.reshape(1, -1), yi)))\n",
    "\n",
    "print(\"\\nProsjecna greska pomocu hinge: {}\\nProsjecna greska pomocu ugradene funkcije: {}\\n\".format(\n",
    "    sum([hinge(svc, seven_X[i].reshape(1,-1), seven_y[i]) for i in range(len(seven_X))]) / len(seven_X),\n",
    "    hinge_loss(seven_y, svc.decision_function(seven_X))))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### (c)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Vratit ćemo se na skupove podataka `outlier` ($N=8$) i `unsep` ($N=8$) iz prošle laboratorijske vježbe (dani niže) i pogledati kako se model SVM-a nosi s njima. Naučite ugrađeni model SVM-a (s linearnom jezgrom) na ovim podatcima i iscrtajte decizijsku granicu (skupa s marginom). Također ispišite točnost modela korištenjem funkcije [`metrics.accuracy_score`](http://scikit-learn.org/stable/modules/generated/sklearn.metrics.accuracy_score.html)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import accuracy_score\n",
    "\n",
    "outlier_X = np.append(seven_X, [[12,8]], axis=0)\n",
    "outlier_y = np.append(seven_y, -1)\n",
    "\n",
    "unsep_X = np.append(seven_X, [[2,2]], axis=0)\n",
    "unsep_y = np.append(seven_y, -1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "svc_o = SVC(kernel=\"linear\").fit(outlier_X, outlier_y)\n",
    "score_o = accuracy_score(outlier_y, svc_o.predict(outlier_X))\n",
    "svc_u = SVC(kernel=\"linear\").fit(unsep_X, unsep_y)\n",
    "score_u = accuracy_score(unsep_y, svc_u.predict(unsep_X))\n",
    "print(\"Model outlier tocnost predikcije: {}\\nModel unsep tocnost predikcije: {}\\n\\n\".format(score_o, score_u))\n",
    "\n",
    "figure(num=None, figsize=(18, 6), dpi=60, facecolor='w', edgecolor='k')\n",
    "subplots_adjust(wspace=0.1)\n",
    "subplot(1, 2, 1)\n",
    "title(\"Outlier\")\n",
    "plot_2d_svc_problem(outlier_X, outlier_y, svc_o)\n",
    "subplot(1, 2, 2)\n",
    "title(\"Unsep\")\n",
    "plot_2d_svc_problem(unsep_X, unsep_y, svc_u)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Q:** Kako stršeća vrijednost utječe na SVM?  \n",
    "**Q:** Kako se linearan SVM nosi s linearno neodvojivim skupom podataka?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2. Nelinearan SVM"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Ovaj zadatak pokazat će kako odabir jezgre utječe na kapacitet SVM-a. Na skupu `unsep` iz prošlog zadatka trenirajte tri modela SVM-a s različitim jezgrenim funkcijama: linearnom, polinomijalnom i radijalnom baznom (RBF) funkcijom. Varirajte hiperparametar $C$ po vrijednostima $C\\in\\{10^{-2},1,10^2\\}$, dok za ostale hiperparametre (stupanj polinoma za polinomijalnu jezgru odnosno hiperparametar $\\gamma$ za jezgru RBF) koristite podrazumijevane vrijednosti. Prikažite granice između klasa (i margine) na grafikonu organiziranome u polje $3x3$, gdje su stupci različite jezgre, a retci različite vrijednosti parametra $C$."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "C_lista = [10**-2, 1, 10**2]\n",
    "jezgre = [\"linear\", \"poly\", \"rbf\"]\n",
    "\n",
    "figure(num=None, figsize=(22, 14), dpi=60, facecolor='w', edgecolor='k')\n",
    "subplots_adjust(wspace=0.1)\n",
    "for i in range(len(C_lista)):\n",
    "    for j in range(len(jezgre)):\n",
    "        subplot(3, 3, i * len(C_lista) + j + 1)\n",
    "        title(\"c={}, {}\".format(C_lista[i], jezgre[j]))\n",
    "        plot_2d_svc_problem(unsep_X, unsep_y, SVC(kernel=jezgre[j], C=C_lista[i]).fit(outlier_X, outlier_y))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3. Optimizacija hiperparametara SVM-a"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Pored hiperparametra $C$, model SVM s jezgrenom funkcijom RBF ima i dodatni hiperparametar $\\gamma=\\frac{1}{2\\sigma^2}$ (preciznost). Taj parametar također određuje složenost modela: velika vrijednost za $\\gamma$ znači da će RBF biti uska, primjeri će biti preslikani u prostor u kojem su (prema skalarnome produktu) međusobno vrlo različiti, što će rezultirati složenijim modelima. Obrnuto, mala vrijednost za $\\gamma$ znači da će RBF biti široka, primjeri će biti međusobno sličniji, što će rezultirati jednostavnijim modelima. To ujedno znači da, ako odabremo veći $\\gamma$, trebamo jače regularizirati model, tj. trebamo odabrati manji $C$, kako bismo spriječili prenaučenost. Zbog toga je potrebno zajednički optimirati hiperparametre $C$ i $\\gamma$, što se tipično radi iscrpnim pretraživanjem po rešetci (engl. *grid search*). Ovakav pristup primjenjuje se kod svih modela koji sadrže više od jednog hiperparametra.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### (a)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Definirajte funkciju \n",
    "\n",
    "> `grid_search(X_train, X_validate, y_train, y_validate, c_range=(c1,c2), g_range=(g1,g2), error_surface=False)` \n",
    "\n",
    "koja optimizira parametre $C$ i $\\gamma$ pretraživanjem po rešetci. Funkcija treba pretražiti hiperparametre $C\\in\\{2^{c_1},2^{c_1+1},\\dots,2^{c_2}\\}$ i $\\gamma\\in\\{2^{g_1},2^{g_1+1},\\dots,2^{g_2}\\}$. Funkcija treba vratiti optimalne hiperparametre $(C^*,\\gamma^*)$, tj. one za koje na skupu za provjeru model ostvaruju najmanju pogrešku. Dodatno, ako je `surface=True`, funkcija treba vratiti matrice (tipa `ndarray`) pogreške modela (očekivanje gubitka 0-1) na skupu za učenje i skupu za provjeru. Svaka je matrica dimenzija $(c_2-c_1+1)\\times(g_2-g_1+1)$ (retci odgovaraju različitim vrijednostima za $C$, a stupci različitim vrijednostima za $\\gamma$)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import accuracy_score, zero_one_loss\n",
    "\n",
    "gen = lambda gen_range: [2**(k) for k in range(gen_range[0], gen_range[1]+1)]\n",
    "\n",
    "def grid_search(X_train, X_validate, y_train, y_validate, c_range=(0,5), g_range=(0,5), error_surface=False):\n",
    "    err_minimum = None\n",
    "    best_c, best_g = -1, -1\n",
    "\n",
    "    err_model_train = np.zeros((c_range[1]-c_range[0]+1, g_range[1]-g_range[0]+1))\n",
    "    err_model_validate = np.zeros((c_range[1]-c_range[0]+1, g_range[1]-g_range[0]+1))\n",
    "    for index_i, i in enumerate(gen(c_range)):\n",
    "        for index_j, j in enumerate(gen(g_range)):\n",
    "            svc = SVC(C=i, gamma=j).fit(X_train, y_train)\n",
    "            pred1 = svc.predict(X_validate)\n",
    "            err = accuracy_score(y_validate, pred1)\n",
    "\n",
    "            if err_minimum is None or err_minimum > err:\n",
    "                err_minimum = err\n",
    "                best_c = i\n",
    "                best_g = j\n",
    "            \n",
    "            if error_surface:\n",
    "                err_model_validate[index_i][index_j] = err\n",
    "                pred2 = svc.predict(X_train)\n",
    "                err_train = accuracy_score(y_train, pred2)\n",
    "                err_model_train[index_i][index_j] = err_train\n",
    "\n",
    "    return (best_c, best_g, err_model_train, err_model_validate) if error_surface else (best_c, best_g)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### (b)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Pomoću funkcije [`datasets.make_classification`](http://scikit-learn.org/stable/modules/generated/sklearn.datasets.make_classification.html) generirajte **dva** skupa podataka od $N=200$ primjera: jedan s $n=2$ dimenzije i drugi s $n=100$ dimenzija. Primjeri neka dolaze iz dviju klasa, s time da svakoj klasi odgovaraju dvije grupe (`n_clusters_per_class=2`), kako bi problem bio nešto složeniji, tj. nelinearniji. Neka sve značajke budu informativne. Podijelite skup primjera na skup za učenje i skup za ispitivanje u omjeru 1:1.\n",
    "\n",
    "Na oba skupa optimirajte SVM s jezgrenom funkcijom RBF, u rešetci $C\\in\\{2^{-5},2^{-4},\\dots,2^{15}\\}$ i $\\gamma\\in\\{2^{-15},2^{-14},\\dots,2^{3}\\}$. Prikažite površinu pogreške modela na skupu za učenje i skupu za provjeru, i to na oba skupa podataka (ukupno četiri grafikona) te ispišite optimalne kombinacije hiperparametara. Za prikaz površine pogreške modela možete koristiti funkciju `mlutils.plot_error_surface`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.datasets import make_classification\n",
    "from sklearn.model_selection import train_test_split\n",
    "import mlutils\n",
    "\n",
    "omjer = 0.5\n",
    "N = 200\n",
    "n1 = 2\n",
    "n2 = 100\n",
    "trainset1_X, trainset1_y = make_classification(n_clusters_per_class=2, n_samples=N, n_features=n1 , n_redundant=0)\n",
    "trainset2_X, trainset2_y = make_classification(n_clusters_per_class=2, n_samples=N, n_features=n2 , n_redundant=0)\n",
    "t1x_train, t1x_validate, t1y_train, t1y_validate = train_test_split(trainset1_X, trainset1_y, test_size=omjer)\n",
    "t2x_train, t2x_validate, t2y_train, t2y_validate = train_test_split(trainset2_X, trainset2_y, test_size=omjer)\n",
    "\n",
    "c1, g1, err_train1, err_validate1 = grid_search(t1x_train, t1x_validate, t1y_train, t1y_validate, c_range=(-5, 15), g_range=(-15,3), error_surface=True)\n",
    "c2, g2, err_train2, err_validate2 = grid_search(t2x_train, t2x_validate, t2y_train, t2y_validate, c_range=(-5, 15), g_range=(-15,3), error_surface=True)\n",
    "\n",
    "figure(num=None, figsize=(22, 14), dpi=60, facecolor='w', edgecolor='k')\n",
    "subplots_adjust(wspace=0.1, hspace = 0.3)\n",
    "subplot(2, 2, 1)\n",
    "plot_error_surface(err_train1, (-5, 15), (-15, 3))\n",
    "subplot(2, 2, 2)\n",
    "plot_error_surface(err_validate1, (-5, 15), (-15, 3))\n",
    "subplot(2, 2, 3)\n",
    "plot_error_surface(err_train2, (-5, 15), (-15, 3))\n",
    "subplot(2, 2, 4)\n",
    "plot_error_surface(err_validate2, (-5, 15), (-15, 3))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Q:** Razlikuje li se površina pogreške na skupu za učenje i skupu za ispitivanje? Zašto?  \n",
    "**Q:** U prikazu površine pogreške, koji dio površine odgovara prenaučenosti, a koji podnaučenosti? Zašto?  \n",
    "**Q:** Kako broj dimenzija $n$ utječe na površinu pogreške, odnosno na optimalne hiperparametre $(C^*, \\gamma^*)$?  \n",
    "**Q:** Preporuka je da povećanje vrijednosti za $\\gamma$ treba biti popraćeno smanjenjem vrijednosti za $C$. Govore li vaši rezultati u prilog toj preporuci? Obrazložite.  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4. Utjecaj standardizacije značajki kod SVM-a"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "U prvoj laboratorijskoj vježbi smo pokazali kako značajke različitih skala mogu onemogućiti interpretaciju naučenog modela linearne regresije. Međutim, ovaj problem javlja se kod mnogih modela pa je tako skoro uvijek bitno prije treniranja skalirati značajke, kako bi se spriječilo da značajke s većim numeričkim rasponima dominiraju nad onima s manjim numeričkim rasponima. To vrijedi i za SVM, kod kojega skaliranje nerijetko može znatno poboljšati rezultate. Svrha ovog zadataka jest eksperimentalno utvrditi utjecaj skaliranja značajki na točnost SVM-a.\n",
    "\n",
    "Generirat ćemo dvoklasni skup od $N=500$ primjera s $n=2$ značajke, tako da je dimenzija $x_1$ većeg iznosa i većeg raspona od dimenzije $x_0$, te ćemo dodati jedan primjer koji vrijednošću značajke $x_1$ odskače od ostalih primjera:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.datasets import make_classification\n",
    "\n",
    "X, y = make_classification(n_samples=500,n_features=2,n_classes=2,n_redundant=0,n_clusters_per_class=1, random_state=69)\n",
    "X[:,1] = X[:,1]*100+1000\n",
    "X[0,1] = 3000\n",
    "\n",
    "plot_2d_svc_problem(X, y)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### (a)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Proučite funkciju za iscrtavanje histograma [`hist`](http://matplotlib.org/api/pyplot_api.html#matplotlib.pyplot.hist). Prikažite histograme vrijednosti značajki $x_0$ i $x_1$ (ovdje i u sljedećim zadatcima koristite `bins=50`)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "x0 = X[:,0]\n",
    "x1 = X[:,1]\n",
    "figure(num=None, figsize=(8, 6), dpi=60, facecolor='w', edgecolor='k')\n",
    "title(\"x0\")\n",
    "hist(x0, bins=50, histtype=\"bar\")\n",
    "figure(num=None, figsize=(8, 6), dpi=60, facecolor='w', edgecolor='k')\n",
    "title(\"x1\")\n",
    "hist(x1, bins=50, histtype=\"bar\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### (b)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Proučite razred [`preprocessing.MinMaxScaler`](http://scikit-learn.org/stable/modules/generated/sklearn.preprocessing.MinMaxScaler.html). Prikažite histograme vrijednosti značajki $x_0$ i $x_1$ ako su iste skalirane min-max skaliranjem (ukupno dva histograma)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.preprocessing import MinMaxScaler\n",
    "x0_ = MinMaxScaler(copy=True).fit_transform(x0.reshape(-1, 1))\n",
    "x1_ = MinMaxScaler(copy=True).fit_transform(x1.reshape(-1, 1))\n",
    "\n",
    "figure(num=None, figsize=(8, 6), dpi=60, facecolor='w', edgecolor='k')\n",
    "title(\"x0'\")\n",
    "hist(x0_, bins=50, histtype=\"bar\")\n",
    "figure(num=None, figsize=(8, 6), dpi=60, facecolor='w', edgecolor='k')\n",
    "title(\"x1'\")\n",
    "hist(x1_, bins=50, histtype=\"bar\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Q:** Kako radi ovo skaliranje? <br>\n",
    "**Q:** Dobiveni histogrami su vrlo slični. U čemu je razlika? <br>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### (c)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Proučite razred [`preprocessing.StandardScaler`](http://scikit-learn.org/stable/modules/generated/sklearn.preprocessing.StandardScaler.html). Prikažite histograme vrijednosti značajki $x_0$ i $x_1$ ako su iste skalirane standardnim skaliranjem (ukupno dva histograma)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.preprocessing import StandardScaler\n",
    "\n",
    "std = StandardScaler(copy=True)\n",
    "x0_ = std.fit_transform(x0.reshape(-1, 1))\n",
    "x1_ = std.fit_transform(x1.reshape(-1, 1))\n",
    "\n",
    "figure(num=None, figsize=(8, 6), dpi=60, facecolor='w', edgecolor='k')\n",
    "title(\"x0'\")\n",
    "hist(x0_, bins=50, histtype=\"bar\")\n",
    "figure(num=None, figsize=(8, 6), dpi=60, facecolor='w', edgecolor='k')\n",
    "title(\"x1'\")\n",
    "hist(x1_, bins=50, histtype=\"bar\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Q:** Kako radi ovo skaliranje? <br>\n",
    "**Q:** Dobiveni histogrami su vrlo slični. U čemu je razlika? <br>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### (d)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Podijelite skup primjera na skup za učenje i skup za ispitivanje u omjeru 1:1. Trenirajte SVM s jezgrenom funkcijom RBF na skupu za učenje i ispitajte točnost modela na skupu za ispitivanje, koristeći tri varijante gornjeg skupa: neskalirane značajke, standardizirane značajke i min-max skaliranje. Koristite podrazumijevane vrijednosti za $C$ i $\\gamma$. Izmjerite točnost svakog od triju modela na skupu za učenje i skupu za ispitivanje. Ponovite postupak više puta (npr. 30) te uprosječite rezultate (u svakom ponavljanju generirajte podatke kao što je dano na početku ovog zadatka).\n",
    "\n",
    "**NB:** Na skupu za učenje treba najprije izračunati parametre skaliranja te zatim primijeniti skaliranje (funkcija `fit_transform`), dok na skupu za ispitivanje treba samo primijeniti skaliranje s parametrima koji su dobiveni na skupu za učenje (funkcija `transform`)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "n = 30\n",
    "omjer = 0.5\n",
    "\n",
    "tocnost_neskaliran = 0.0\n",
    "tocnost_minmax = 0.0\n",
    "tocnost_std = 0.0\n",
    "for i in range(n):\n",
    "    x_train, x_validate, y_train, y_validate = train_test_split(X, y, test_size=omjer)\n",
    "\n",
    "    svc_neskaliran = SVC(kernel=\"rbf\").fit(x_train, y_train)\n",
    "    tocnost_neskaliran += accuracy_score(y_validate, svc_neskaliran.predict(x_validate))\n",
    "    \n",
    "    minmax = MinMaxScaler(copy=True)\n",
    "    std = StandardScaler(copy=True)\n",
    "    \n",
    "    x_train_minmax = minmax.fit_transform(x_train, y_train)\n",
    "    svc_minmax = SVC(kernel=\"rbf\").fit(x_train_minmax, y_train)\n",
    "    x_validate_minmax = minmax.transform(x_validate)\n",
    "    tocnost_minmax += accuracy_score(y_validate, svc_minmax.predict(x_validate_minmax))\n",
    "\n",
    "    x_train_std = std.fit_transform(x_train, y_train)\n",
    "    svc_std = SVC(kernel=\"rbf\").fit(x_train_std, y_train)\n",
    "    x_validate_std = std.transform(x_validate)\n",
    "    tocnost_std += accuracy_score(y_validate, svc_std.predict(x_validate_std))\n",
    "\n",
    "print(\"Prosječna greška za neskalirane podatke: {}\\nProsječna greška za podatke skalirane pomoću minmax: {}\\nProsječna greška za podatke skalirane pomoću standard scalera: {}\".format(\n",
    "    1 - tocnost_neskaliran / n,\n",
    "    1 - tocnost_minmax / n,\n",
    "    1 - tocnost_std / n))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Q:** Jesu li rezultati očekivani? Obrazložite. <br>\n",
    "**Q:** Bi li bilo dobro kada bismo funkciju `fit_transform` primijenili na cijelom skupu podataka? Zašto? Bi li bilo dobro kada bismo tu funkciju primijenili zasebno na skupu za učenje i zasebno na skupu za ispitivanje? Zašto?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 5. Algoritam k-najbližih susjeda"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "U ovom zadatku promatrat ćemo jednostavan klasifikacijski model imena **algoritam k-najbližih susjeda**. Najprije ćete ga samostalno isprogramirati kako biste se detaljno upoznali s radom ovog modela, a zatim ćete prijeći na analizu njegovih hiperparametara (koristeći ugrađeni razred, radi efikasnosti)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### (a)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Implementirajte klasu `KNN`, koja implementira algoritam $k$ najbližih susjeda. Neobavezan parametar konstruktora jest broj susjeda `n_neighbours` ($k$), čija je podrazumijevana vrijednost 3. Definirajte metode `fit(X, y)` i `predict(X)`, koje služe za učenje modela odnosno predikciju. Kao mjeru udaljenosti koristite euklidsku udaljenost ([`numpy.linalg.norm`](https://numpy.org/doc/stable/reference/generated/numpy.linalg.norm.html); pripazite na parametar `axis`). Nije potrebno implementirati nikakvu težinsku funkciju."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from numpy.linalg import norm\n",
    "from collections import Counter\n",
    "\n",
    "class KNN:\n",
    "    x_list = []\n",
    "    y_list = []\n",
    "    d = []\n",
    "\n",
    "    def __init__(self, n_neighbors=3):\n",
    "        self.n_neighbors = n_neighbors\n",
    "                \n",
    "    def fit(self, X_train, y_train):\n",
    "        for x, y in zip(X_train, y_train):\n",
    "            self.x_list.append(x)\n",
    "            self.y_list.append(y)\n",
    "            self.d.append((x, y))\n",
    "        return self\n",
    "        \n",
    "    def predict(self, X_test):\n",
    "        ret = []\n",
    "        for x in X_test:\n",
    "            norme = [norm(xi-x) for xi in self.x_list]\n",
    "            sortirano = sorted(list(zip(norme, self.y_list)), key=lambda x: x[0])\n",
    "            \n",
    "            c = {}\n",
    "            for i in range(self.n_neighbors):\n",
    "                pair = sortirano[i]\n",
    "                if pair[1] in c:\n",
    "                    c[pair[1]] += 1\n",
    "                else:\n",
    "                    c[pair[1]] = 1 \n",
    "\n",
    "            max_pair = (-5, -inf)\n",
    "            for key, val in c.items():\n",
    "                if abs(val) > max_pair[1]:\n",
    "                    max_pair = (key, val)\n",
    "            ret.append(max_pair[0])\n",
    "        return np.array(ret)\n",
    "        "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### (b)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Kako biste se uvjerili da je Vaša implementacija ispravna, usporedite ju s onom u razredu [`neighbors.KNeighborsClassifier`](http://scikit-learn.org/stable/modules/generated/sklearn.neighbors.KNeighborsClassifier.html). Budući da spomenuti razred koristi razne optimizacijske trikove pri pronalasku najboljih susjeda, obavezno postavite parametar `algorithm=brute`, jer bi se u protivnom moglo dogoditi da vam se predikcije razlikuju. Usporedite modele na danom (umjetnom) skupu podataka (prisjetite se kako se uspoređuju polja; [`numpy.all`](https://numpy.org/doc/stable/reference/generated/numpy.all.html))."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.datasets import make_classification\n",
    "X_art, y_art = make_classification(n_samples=100, n_features=2, n_classes=2, \n",
    "                                   n_redundant=0, n_clusters_per_class=2,\n",
    "                                   random_state=69)\n",
    "plot_2d_clf_problem(X_art, y_art)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "\n",
    "knn1 = KNN().fit(X_art, y_art)\n",
    "pred1 = knn1.predict(X_art)\n",
    "print(\"Moj model:\\n{}\\n\\n\".format(pred1))\n",
    "\n",
    "knn2 = KNeighborsClassifier(algorithm=\"brute\", n_neighbors=3).fit(X_art, y_art)\n",
    "pred2 = knn2.predict(X_art)\n",
    "print(\"Ugrađeni model:\\n{}\\n\\n\".format(pred2))\n",
    "\n",
    "razl = 0\n",
    "for y1, y2 in zip(pred1, pred2):\n",
    "    razl += 1 if y1 != y2 else 0\n",
    "print(\"Broj klasifikacija koje se razlikuju: {}\".format(razl))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 6. Analiza algoritma k-najbližih susjeda"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Algoritam k-nn ima hiperparametar $k$ (broj susjeda). Taj hiperparametar izravno utječe na složenost algoritma, pa je stoga izrazito važno dobro odabrati njegovu vrijednost. Kao i kod mnogih drugih algoritama, tako i kod algoritma k-nn optimalna vrijednost hiperametra $k$ ovisi o konkretnom problemu, uključivo broju primjera $N$, broju značajki (dimenzija) $n$ te broju klasa $K$. \n",
    "\n",
    "Kako bismo dobili pouzdanije rezultate, potrebno je neke od eksperimenata ponoviti na različitim skupovima podataka i zatim uprosječiti dobivene vrijednosti pogrešaka. Koristite funkciju: `knn_eval` koja trenira i ispituje model k-najbližih susjeda na ukupno `n_instances` primjera, i to tako da za svaku vrijednost hiperparametra iz zadanog intervala `k_range` ponovi `n_trials` mjerenja, generirajući za svako od njih nov skup podataka i dijeleći ga na skup za učenje i skup za ispitivanje. Udio skupa za ispitivanje definiran je parametrom `test_size`. Povratna vrijednost funkcije jest četvorka `(ks, best_k, train_errors, test_errors)`. Vrijednost `best_k` je optimalna vrijednost hiperparametra $k$ (vrijednost za koju je pogreška na skupu za ispitivanje najmanja). Vrijednosti `train_errors` i `test_errors`  liste su pogrešaka na skupu za učenja odnosno skupu za testiranje za sve razmatrane vrijednosti hiperparametra $k$, dok `ks` upravo pohranjuje sve razmatrane vrijednosti hiperparametra $k$."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### (a)\n",
    "\n",
    "Na podatcima iz zadatka 5, pomoću funkcije `plot_2d_clf_problem` iscrtajte prostor primjera i područja koja odgovaraju prvoj odnosno drugoj klasi. Ponovite ovo za $k\\in[1, 5, 20, 100]$. \n",
    "\n",
    "**NB:** Implementacija algoritma `KNeighborsClassifier` iz paketa `scikit-learn` vjerojatno će raditi brže od Vaše implementacije, pa u preostalim eksperimentima koristite nju."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "k_lista = [1, 5, 20, 100]\n",
    "for i, k in enumerate(k_lista):\n",
    "    X_art, y_art = make_classification(n_samples=100, n_features=2, n_classes=2, \n",
    "                                   n_redundant=0, n_clusters_per_class=2,\n",
    "                                   random_state=69)\n",
    "    knn = KNeighborsClassifier(algorithm=\"brute\", n_neighbors=k).fit(X_art, y_art)\n",
    "\n",
    "    figure(num=None, figsize=(8, 6), dpi=60, facecolor='w', edgecolor='k')\n",
    "    title(\"k={}\".format(k))\n",
    "    plot_2d_clf_problem(X_art, y_art, knn.predict)\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Q:** Kako $k$ utječe na izgled granice između klasa?  \n",
    "**Q:** Kako se algoritam ponaša u ekstremnim situacijama: $k=1$ i $k=100$?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### (b)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Pomoću funkcije `knn_eval`, iscrtajte pogreške učenja i ispitivanja kao funkcije hiperparametra $k\\in\\{1,\\dots,20\\}$, za $N=\\{100, 250, 750\\}$ primjera. Načinite 3 zasebna grafikona. Za svaki ispišite optimalnu vrijednost hiperparametra $k$ (najlakše kao naslov grafikona; vidi [`plt.title`](https://matplotlib.org/devdocs/api/_as_gen/matplotlib.pyplot.title.html))."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "k_range = range(1, 21)\n",
    "N = [100, 250, 750]\n",
    "\n",
    "for n in N:\n",
    "    (ks, best_k, train_errors, test_errors) = knn_eval(n_instances=n, k_range=(1, 20))\n",
    "    figure(num=None, figsize=(8, 6), dpi=60, facecolor='w', edgecolor='k')\n",
    "    title(\"Najbolji model se dobiva za k={}\".format(best_k))\n",
    "    plot(np.array(k_range), train_errors)\n",
    "    plot(np.array(k_range), test_errors)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Q:** Kako se mijenja optimalna vrijednost hiperparametra $k$ s obzirom na broj primjera $N$? Zašto?  \n",
    "**Q:** Kojem području odgovara prenaučenost, a kojem podnaučenost modela? Zašto?  \n",
    "**Q:** Je li uvijek moguće doseći pogrešku od 0 na skupu za učenje?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### (c)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Kako bismo provjerili u kojoj je mjeri algoritam k-najbližih susjeda osjetljiv na prisustvo nebitnih značajki, možemo iskoristiti funkciju [`datasets.make_classification`](http://scikit-learn.org/stable/modules/generated/sklearn.datasets.make_classification.html) kako bismo generirali skup primjera kojemu su neke od značajki nebitne. Naime, parametar `n_informative` određuje broj bitnih značajki, dok parametar `n_features` određuje ukupan broj značajki. Ako je `n_features > n_informative`, onda će neke od značajki biti nebitne. Umjesto da izravno upotrijebimo funkciju `make_classification`, upotrijebit ćemo funkciju `knn_eval`, koja samo preuzime ove parametre, ali nam omogućuje pouzdanije procjene.\n",
    "\n",
    "Koristite funkciju `mlutils.knn_eval` na dva načina. U oba koristite $N=1000$ primjera, $n=10$ značajki i $K=5$ klasa, ali za prvi neka su svih 10 značajki bitne, a za drugi neka je bitno samo 5 od 10 značajki. Ispišite pogreške učenja i ispitivanja za oba modela za optimalnu vrijednost $k$ (vrijednost za koju je ispitna pogreška najmanja)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "N = 1000\n",
    "n = 10\n",
    "K = 5\n",
    "\n",
    "i = 10\n",
    "(ks, best_k, train_errors, test_errors) = knn_eval(n_instances=N, n_features=n, n_classes=K, n_informative=i)\n",
    "figure(num=None, figsize=(8, 6), dpi=60, facecolor='w', edgecolor='k')\n",
    "title(\"Najbolji model se dobiva za k={}\".format(best_k))\n",
    "plot(np.array(k_range), train_errors)\n",
    "plot(np.array(k_range), test_errors)\n",
    "plt.show()\n",
    "\n",
    "i = 5\n",
    "(ks, best_k, train_errors, test_errors) = knn_eval(n_instances=N, n_features=n, n_classes=K, n_informative=i)\n",
    "figure(num=None, figsize=(8, 6), dpi=60, facecolor='w', edgecolor='k')\n",
    "title(\"Najbolji model se dobiva za k={}\".format(best_k))\n",
    "plot(np.array(k_range), train_errors)\n",
    "plot(np.array(k_range), test_errors)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Q:** Je li algoritam k-najbližih susjeda osjetljiv na nebitne značajke? Zašto?  \n",
    "**Q:** Je li ovaj problem izražen i kod ostalih modela koje smo dosad radili (npr. logistička regresija)?  \n",
    "**Q:** Kako bi se model k-najbližih susjeda ponašao na skupu podataka sa značajkama različitih skala? Detaljno pojasnite."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 7. \"Prokletstvo dimenzionalnosti\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\"Prokletstvo dimenzionalnosti\" zbirni je naziv za niz fenomena povezanih s visokodimenzijskim prostorima. Ti fenomeni, koji se uglavnom protive našoj intuiciji, u većini slučajeva dovode do toga da se s porastom broja dimenzija (značajki) smanjenje točnost modela."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Općenito, povećanje dimenzija dovodi do toga da sve točke u ulaznome prostoru postaju (u smislu euklidske udaljenosti) sve udaljenije jedne od drugih te se, posljedično, gube razlike u udaljenostima između točaka. Eksperimentalno ćemo provjeriti da je to doista slučaj. Proučite funkciju [`metrics.pairwise_distances`](http://scikit-learn.org/stable/modules/generated/sklearn.metrics.pairwise_distances.html). Generirajte 100 slučajnih vektora u različitim dimenzijama $n\\in[1,2,\\ldots,50]$ dimenzija te izračunajte *prosječnu* euklidsku udaljenost između svih parova tih vektora. Za generiranje slučajnih vektora koristite funkciju [`numpy.random.random`](https://numpy.org/doc/stable/reference/random/generated/numpy.random.random.html). Na istom grafu skicirajte i krivulju za prosječne kosinusne udaljenosti (parametar `metric`)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics.pairwise import pairwise_distances\n",
    "\n",
    "n = range(1, 51)\n",
    "N = 100\n",
    "\n",
    "distances = []\n",
    "cos_distances = []\n",
    "for i in n:\n",
    "    vectors = []\n",
    "    for br_vektora in range(N):\n",
    "        vectors.append(np.random.random(size=i))\n",
    "    cur_d = mean(pairwise_distances(vectors, metric=\"euclidean\"))\n",
    "    cos_d = mean(pairwise_distances(vectors, metric=\"cosine\"))\n",
    "    distances.append(cur_d)\n",
    "    cos_distances.append(cos_d)\n",
    "\n",
    "figure(num=None, figsize=(8, 6), dpi=60, facecolor='w', edgecolor='k')\n",
    "plot(np.array(n), distances)\n",
    "plot(np.array(n), cos_distances)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Q:** Pokušajte objasniti razlike u rezultatima. Koju biste od ovih dviju mjera koristili za klasifikaciju visokodimenzijskih podataka?  \n",
    "**Q:** Zašto je ovaj problem osobito izražen kod algoritma k-najbližih susjeda?"
   ]
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.9-final"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}